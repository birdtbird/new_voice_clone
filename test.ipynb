{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('tortoise-tts')\n",
    "\n",
    "from tortoise.api import TextToSpeech\n",
    "from tortoise.utils.audio import load_audio, load_voice, load_voices\n",
    "import imageio\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class genreator:\n",
    "  def __init__(self,preset =\"fast\" ) -> None:\n",
    "    # Pick a \"preset mode\" to determine quality. Options: {\"ultra_fast\", \"fast\" (default), \"standard\", \"high_quality\"}. See docs in api.py\n",
    "\n",
    "    self.tts = TextToSpeech()\n",
    "    self.preset = preset\n",
    "    self.model_path ='/content/Audio2Head/checkpoints/audio2head.pth.tar'\n",
    "\n",
    "  def gen_vdo(self,voice,img_path,text):\n",
    "    temp_audio_path = './temp/generated.wav'\n",
    "    voice_samples, conditioning_latents = load_voice(voice)\n",
    "    voice_gen = self.tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents,preset=self.preset)\n",
    "    torchaudio.save(temp_audio_path, voice_gen.squeeze(0).cpu(), 24000)\n",
    "    vdo_gen = audio2head(temp_audio_path,img_path,self.model_path)\n",
    "    video_path = './temp/vdo.mp4'\n",
    "    temp_path ='temp/vdo_temp.mp4'\n",
    "    imageio.mimsave(temp_path, vdo_gen, fps=25.0)\n",
    "    cmd = r'ffmpeg -y -i \"%s\" -i \"%s\" -vcodec copy \"%s\"' % (temp_path, temp_audio_path, video_path)\n",
    "    os.system(cmd)\n",
    "    os.remove(temp_path)\n",
    "    vid = imageio.get_reader(video_path,  'ffmpeg')\n",
    "    return video_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\0_project\\AIT_HACK\\headtalking_voice_clone\\tortoise-tts\n"
     ]
    }
   ],
   "source": [
    "%cd tortoise-tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\0_project\\AIT_HACK\\headtalking_voice_clone\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genreator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating autoregressive samples..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:13<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing best candidates using CLVP and CVVP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]c:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "  0%|          | 0/6 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 6.00 GiB total capacity; 4.97 GiB already allocated; 0 bytes free; 5.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/content/Audio2Head/demo/img/baiden.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      3\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhi test test my name is tom.how are you doing\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m video_path \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgen_vdo(voice,img_path,text)\n",
      "Cell \u001b[1;32mIn [4], line 12\u001b[0m, in \u001b[0;36mgenreator.gen_vdo\u001b[1;34m(self, voice, img_path, text)\u001b[0m\n\u001b[0;32m     10\u001b[0m temp_audio_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./temp/generated.wav\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     11\u001b[0m voice_samples, conditioning_latents \u001b[39m=\u001b[39m load_voice(voice)\n\u001b[1;32m---> 12\u001b[0m voice_gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtts\u001b[39m.\u001b[39;49mtts_with_preset(text, voice_samples\u001b[39m=\u001b[39;49mvoice_samples, conditioning_latents\u001b[39m=\u001b[39;49mconditioning_latents,preset\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreset)\n\u001b[0;32m     13\u001b[0m torchaudio\u001b[39m.\u001b[39msave(temp_audio_path, voice_gen\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu(), \u001b[39m24000\u001b[39m)\n\u001b[0;32m     14\u001b[0m vdo_gen \u001b[39m=\u001b[39m audio2head(temp_audio_path,img_path,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_path)\n",
      "File \u001b[1;32me:\\0_project\\AIT_HACK\\headtalking_voice_clone\\tortoise-tts\\tortoise\\api.py:289\u001b[0m, in \u001b[0;36mTextToSpeech.tts_with_preset\u001b[1;34m(self, text, preset, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m presets \u001b[39m=\u001b[39m {\n\u001b[0;32m    283\u001b[0m     \u001b[39m'\u001b[39m\u001b[39multra_fast\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mnum_autoregressive_samples\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m16\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdiffusion_iterations\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m30\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcond_free\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mFalse\u001b[39;00m},\n\u001b[0;32m    284\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfast\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mnum_autoregressive_samples\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m96\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdiffusion_iterations\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m80\u001b[39m},\n\u001b[0;32m    285\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstandard\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mnum_autoregressive_samples\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m256\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdiffusion_iterations\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m200\u001b[39m},\n\u001b[0;32m    286\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mhigh_quality\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mnum_autoregressive_samples\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m256\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdiffusion_iterations\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m400\u001b[39m},\n\u001b[0;32m    287\u001b[0m }\n\u001b[0;32m    288\u001b[0m kwargs\u001b[39m.\u001b[39mupdate(presets[preset])\n\u001b[1;32m--> 289\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtts(text, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\0_project\\AIT_HACK\\headtalking_voice_clone\\tortoise-tts\\tortoise\\api.py:393\u001b[0m, in \u001b[0;36mTextToSpeech.tts\u001b[1;34m(self, text, voice_samples, conditioning_latents, k, verbose, num_autoregressive_samples, temperature, length_penalty, repetition_penalty, top_p, max_mel_tokens, clvp_cvvp_slider, diffusion_iterations, cond_free, cond_free_k, diffusion_temperature, **hf_generate_kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(batch\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m    392\u001b[0m     batch[i] \u001b[39m=\u001b[39m fix_autoregressive_output(batch[i], stop_mel_token)\n\u001b[1;32m--> 393\u001b[0m clvp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclvp(text_tokens\u001b[39m.\u001b[39;49mrepeat(batch\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], \u001b[39m1\u001b[39;49m), batch, return_loss\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    394\u001b[0m \u001b[39mif\u001b[39;00m auto_conds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    395\u001b[0m     cvvp_accumulator \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\0_project\\AIT_HACK\\headtalking_voice_clone\\tortoise-tts\\tortoise\\models\\clvp.py:121\u001b[0m, in \u001b[0;36mCLVP.forward\u001b[1;34m(self, text, speech_tokens, return_loss)\u001b[0m\n\u001b[0;32m    118\u001b[0m     speech_emb \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspeech_pos_emb(torch\u001b[39m.\u001b[39marange(speech_emb\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], device\u001b[39m=\u001b[39mdevice))\n\u001b[0;32m    120\u001b[0m enc_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_transformer(text_emb, mask\u001b[39m=\u001b[39mtext_mask)\n\u001b[1;32m--> 121\u001b[0m enc_speech \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspeech_transformer(speech_emb, mask\u001b[39m=\u001b[39;49mvoice_mask)\n\u001b[0;32m    123\u001b[0m text_latents \u001b[39m=\u001b[39m masked_mean(enc_text, text_mask, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    124\u001b[0m speech_latents \u001b[39m=\u001b[39m masked_mean(enc_speech, voice_mask, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\0_project\\AIT_HACK\\headtalking_voice_clone\\tortoise-tts\\tortoise\\models\\arch_util.py:364\u001b[0m, in \u001b[0;36mCheckpointedXTransformerEncoder.forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_permute:\n\u001b[0;32m    363\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 364\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexit_permute:\n\u001b[0;32m    366\u001b[0m     h \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\0_project\\AIT_HACK\\headtalking_voice_clone\\tortoise-tts\\tortoise\\models\\xtransformers.py:1237\u001b[0m, in \u001b[0;36mContinuousTransformerWrapper.forward\u001b[1;34m(self, x, return_embeddings, mask, return_attn, mems, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m   1234\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_emb(x)\n\u001b[0;32m   1235\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_dropout(x)\n\u001b[1;32m-> 1237\u001b[0m x, intermediates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_layers(x, mask\u001b[39m=\u001b[39;49mmask, mems\u001b[39m=\u001b[39;49mmems, return_hiddens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1238\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[0;32m   1240\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproject_out(x) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_embeddings \u001b[39melse\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\0_project\\AIT_HACK\\headtalking_voice_clone\\tortoise-tts\\tortoise\\models\\xtransformers.py:972\u001b[0m, in \u001b[0;36mAttentionLayers.forward\u001b[1;34m(self, x, context, full_context, mask, context_mask, attn_mask, mems, return_hiddens, norm_scale_shift_inp, past_key_values, expected_seq_len)\u001b[0m\n\u001b[0;32m    969\u001b[0m         layer_past \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[39mif\u001b[39;00m layer_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 972\u001b[0m     out, inter, k, v \u001b[39m=\u001b[39m checkpoint(block, x, \u001b[39mNone\u001b[39;49;00m, mask, \u001b[39mNone\u001b[39;49;00m, attn_mask, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpia_pos_emb, rotary_pos_emb,\n\u001b[0;32m    973\u001b[0m                             prev_attn, layer_mem, layer_past)\n\u001b[0;32m    974\u001b[0m \u001b[39melif\u001b[39;00m layer_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    975\u001b[0m     \u001b[39mif\u001b[39;00m exists(full_context):\n",
      "File \u001b[1;32mc:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\checkpoint.py:235\u001b[0m, in \u001b[0;36mcheckpoint\u001b[1;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected keyword arguments: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m kwargs))\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m use_reentrant:\n\u001b[1;32m--> 235\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[0;32m    238\u001b[0m         function,\n\u001b[0;32m    239\u001b[0m         preserve,\n\u001b[0;32m    240\u001b[0m         \u001b[39m*\u001b[39margs\n\u001b[0;32m    241\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\checkpoint.py:96\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[1;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[0;32m     93\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[0;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 96\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\0_project\\AIT_HACK\\headtalking_voice_clone\\tortoise-tts\\tortoise\\models\\arch_util.py:341\u001b[0m, in \u001b[0;36mCheckpointedLayer.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(v, torch\u001b[39m.\u001b[39mTensor) \u001b[39mand\u001b[39;00m v\u001b[39m.\u001b[39mrequires_grad)  \u001b[39m# This would screw up checkpointing.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m partial \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 341\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mcheckpoint\u001b[39m.\u001b[39;49mcheckpoint(partial, x, \u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\checkpoint.py:235\u001b[0m, in \u001b[0;36mcheckpoint\u001b[1;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected keyword arguments: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m kwargs))\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m use_reentrant:\n\u001b[1;32m--> 235\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[0;32m    238\u001b[0m         function,\n\u001b[0;32m    239\u001b[0m         preserve,\n\u001b[0;32m    240\u001b[0m         \u001b[39m*\u001b[39margs\n\u001b[0;32m    241\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\utils\\checkpoint.py:96\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[1;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[0;32m     93\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[0;32m     95\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 96\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\birdt\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\0_project\\AIT_HACK\\headtalking_voice_clone\\tortoise-tts\\tortoise\\models\\xtransformers.py:658\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x, context, mask, context_mask, attn_mask, sinusoidal_emb, rotary_pos_emb, prev_attn, mem, layer_past)\u001b[0m\n\u001b[0;32m    655\u001b[0m     q, k \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(l2norm, (q, k))\n\u001b[0;32m    656\u001b[0m     scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\u001b[39m.\u001b[39mexp()\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m))\n\u001b[1;32m--> 658\u001b[0m dots \u001b[39m=\u001b[39m einsum(\u001b[39m'\u001b[39;49m\u001b[39mb h i d, b h j d -> b h i j\u001b[39;49m\u001b[39m'\u001b[39;49m, q, k) \u001b[39m*\u001b[39;49m scale\n\u001b[0;32m    659\u001b[0m mask_value \u001b[39m=\u001b[39m max_neg_value(dots)\n\u001b[0;32m    661\u001b[0m \u001b[39mif\u001b[39;00m exists(prev_attn):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 184.00 MiB (GPU 0; 6.00 GiB total capacity; 4.97 GiB already allocated; 0 bytes free; 5.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "voice = 'tom'\n",
    "img_path = '/content/Audio2Head/demo/img/baiden.jpg'\n",
    "text = 'hi test test my name is tom.how are you doing'\n",
    "\n",
    "video_path = model.gen_vdo(voice,img_path,text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c902b2c3016d25595369a18891b7dcffcfcae9b87d6f1a5729dcd08235251b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
